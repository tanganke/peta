,model,dataset,accuracy,config
0,flan-t5-base,glue-stsb,0.7786461174516925,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}"
