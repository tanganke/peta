,model,dataset,accuracy,config
0,flan-t5-base,glue-cola,0.6893576222435283,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}"
1,flan-t5-base,glue-mnli,0.38634742740703004,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}"
2,flan-t5-base,glue-mrpc,0.6838235294117647,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}"
3,flan-t5-base,glue-qqp,0.8033638387336136,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}"
4,flan-t5-base,glue-rte,0.631768953068592,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}"
5,flan-t5-base,glue-sst2,0.9059633027522935,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}"
6,flan-t5-base,glue-stsb,0.056,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}"
