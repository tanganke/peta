Model,Method,Task,Accuracy,version,config,RoLA.r,batch_size,steps,lr,weight_decay
flan-t5-base,FFT,glue-cola,0.695110258868648,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,1e-05,0.0
flan-t5-base,FFT,glue-cola,0.752636625119846,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-cola,0.62320230105465,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-cola,0.688398849472674,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-cola,0.689357622243528,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-cola,0.690316395014381,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,L_LoRA,glue-cola,0.555129434324065,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-cola,0.689357622243528,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-cola,0.486097794822627,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-cola,0.688398849472674,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-cola,0.688398849472674,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-cola,0.691275167785234,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-cola,0.692233940556088,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-cola,0.692233940556088,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-cola,0.692233940556088,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-cola,0.692233940556088,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-cola,0.691275167785234,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-cola,0.692233940556088,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-cola,0.690316395014381,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-cola,0.692233940556088,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-cola,0.691275167785234,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-cola,0.691275167785234,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-cola', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'cola'}, 'preprocessor': {'_target_': 'peta.tasks.CoLA_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,FFT,glue-mnli,0.814161996943453,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,1e-05,0.0
flan-t5-base,FFT,glue-mnli,0.824146714212939,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-mnli,0.00010188487009679,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-mnli,0.290371879775853,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-mnli,0.38634742740703,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-mnli,0.458889454915944,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,L_LoRA,glue-mnli,0.00010188487009679,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-mnli,0.256546102903718,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-mnli,0.00010188487009679,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-mnli,0.288639836984207,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-mnli,0.422312786551197,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-mnli,0.501986754966887,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-mnli,0.666530820173204,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-mnli,0.713092205807437,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-mnli,0.741823739174732,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-mnli,0.761385634233316,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-mnli,0.739582272032603,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-mnli,0.760061130922058,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-mnli,0.668670402445236,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-mnli,0.717575140091696,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-mnli,0.745185939887926,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-mnli,0.764136525725929,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mnli', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mnli'}, 'preprocessor': {'_target_': 'peta.tasks.MNLI_Preprocessor'}, 'map_kwargs': {'remove_columns': ['idx', 'hypothesis', 'premise', 'label'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,FFT,glue-mrpc,0.823529411764705,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,1e-05,0.0
flan-t5-base,FFT,glue-mrpc,0.857843137254901,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-mrpc,0.0,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-mrpc,0.683823529411764,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-mrpc,0.683823529411764,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-mrpc,0.683823529411764,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,L_LoRA,glue-mrpc,0.0,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-mrpc,0.683823529411764,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-mrpc,0.0,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-mrpc,0.683823529411764,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-mrpc,0.683823529411764,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-mrpc,0.683823529411764,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-mrpc,0.723039215686274,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-mrpc,0.781862745098039,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-mrpc,0.79656862745098,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-mrpc,0.803921568627451,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-mrpc,0.799019607843137,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-mrpc,0.799019607843137,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-mrpc,0.700980392156862,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-mrpc,0.762254901960784,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-mrpc,0.791666666666666,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-mrpc,0.799019607843137,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-mrpc', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'mrpc'}, 'preprocessor': {'_target_': 'peta.tasks.MRPC_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,FFT,glue-qqp,0.829334652485777,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,1e-05,0.0
flan-t5-base,FFT,glue-qqp,0.83608706406134,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-qqp,0.0,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-qqp,0.34828097947069,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-qqp,0.803363838733613,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-qqp,0.699035369774919,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,L_LoRA,glue-qqp,0.0,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-qqp,0.334182537719515,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-qqp,0.0,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-qqp,0.471407370764283,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-qqp,0.738238931486519,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-qqp,0.699851595349987,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-qqp,0.734034133069502,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-qqp,0.790774177590897,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-qqp,0.812342320059361,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-qqp,0.824635171902052,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-qqp,0.812441256492703,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-qqp,0.823917882760326,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-qqp,0.735320306702943,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-qqp,0.792406628741033,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-qqp,0.813826366559485,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-qqp,0.825946079643828,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-qqp', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'qqp'}, 'preprocessor': {'_target_': 'peta.tasks.QQP_Preprocessor'}, 'map_kwargs': {'remove_columns': ['question1', 'question2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,FFT,glue-rte,0.851985559566787,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,1e-05,0.0
flan-t5-base,FFT,glue-rte,0.851985559566787,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-rte,0.0,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-rte,0.418772563176895,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-rte,0.631768953068592,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-rte,0.754512635379061,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,L_LoRA,glue-rte,0.0,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-rte,0.472924187725631,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-rte,0.613718411552346,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-rte,0.613718411552346,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-rte,0.743682310469314,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,L_LoRA,glue-rte,0.743682310469314,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-rte,0.826714801444043,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-rte,0.815884476534296,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-rte,0.812274368231046,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-rte,0.801444043321299,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-rte,0.805054151624548,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-rte,0.801444043321299,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-rte,0.805054151624548,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-rte,0.805054151624548,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-rte,0.801444043321299,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-rte,0.801444043321299,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-rte', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'rte'}, 'preprocessor': {'_target_': 'peta.tasks.RTE_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,FFT,glue-sst2,0.93348623853211,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,1e-05,0.0
flan-t5-base,FFT,glue-sst2,0.93348623853211,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-sst2,0.880733944954128,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-sst2,0.910550458715596,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-sst2,0.905963302752293,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-sst2,0.90940366972477,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,L_LoRA,glue-sst2,0.880733944954128,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-sst2,0.912844036697247,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-sst2,0.876146788990825,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-sst2,0.91743119266055,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-sst2,0.910550458715596,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-sst2,0.915137614678899,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-sst2,0.919724770642201,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-sst2,0.922018348623853,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-sst2,0.920871559633027,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-sst2,0.922018348623853,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-sst2,0.919724770642201,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-sst2,0.922018348623853,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-sst2,0.919724770642201,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-sst2,0.923165137614678,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [0, 1, 2, 3], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-sst2,0.923165137614678,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-sst2,0.923165137614678,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-sst2', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'sst2'}, 'preprocessor': {'_target_': 'peta.tasks.SST2_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,FFT,glue-stsb,0.196,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,1e-05,0.0
flan-t5-base,FFT,glue-stsb,0.198666666666666,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': None}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-stsb,0.00533333333333333,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-stsb,0.0286666666666666,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-stsb,0.056,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-stsb,0.0893333333333333,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,L_LoRA,glue-stsb,0.00133333333333333,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-stsb,0.0313333333333333,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-stsb,0.000666666666666666,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,L_LoRA,glue-stsb,0.03,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,L_LoRA,glue-stsb,0.0686666666666666,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,L_LoRA,glue-stsb,0.0946666666666666,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': True}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-stsb,0.140666666666666,4.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-stsb,0.148,5.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-stsb,0.172,6.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-stsb,0.186666666666666,7.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",16.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-stsb,0.173333333333333,8.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-stsb,0.186,9.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",32.0,16.0,2000.0,4e-05,0.0
flan-t5-base,LoRA,glue-stsb,0.144666666666666,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,1e-05,0.0
flan-t5-base,LoRA,glue-stsb,0.149333333333333,,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 2e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,2e-05,0.0
flan-t5-base,LoRA,glue-stsb,0.172666666666666,2.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 3e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,3e-05,0.0
flan-t5-base,LoRA,glue-stsb,0.183333333333333,3.0,"{'model': {'model': {'_target_': 'transformers.AutoModelForSeq2SeqLM.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'tokenizer': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${..model_name_or_path}'}, 'model_name_or_path': 'google/flan-t5-base', 'name': 'flan-t5-base', 'tokenizer_kwargs': {'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}, 'linearize': False}, 'peft': {'peft_config': {'_target_': 'peft.LoraConfig', 'target_modules': ['q', 'v'], 'inference_mode': False, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1}, 'seed': 42}, 'dataset': {'name': 'glue-stsb', 'datasets': {'_target_': 'datasets.load_dataset', 'path': 'glue', 'name': 'stsb'}, 'preprocessor': {'_target_': 'peta.tasks.STSB_Preprocessor'}, 'map_kwargs': {'remove_columns': ['sentence1', 'sentence2', 'label', 'idx'], 'batched': True, 'num_proc': 1, 'desc': 'Running tokenizer on dataset'}}, 'optim': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 4e-05, 'weight_decay': 0}}, 'seed': 42, 'batch_size': 16, 'num_workers': 8, 'trainer': {'accelerator': 'gpu', 'devices': [4, 5, 6, 7], 'max_epochs': None, 'max_steps': 2000, 'accumulate_grad_batches': 1, 'profiler': 'simple', 'enable_checkpointing': False, 'fast_dev_run': False}}",8.0,16.0,2000.0,4e-05,0.0
