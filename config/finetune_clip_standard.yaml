defaults:
  - model: ViT-B-16
  - dataset: Cars
  - _self_

seed: 42
fast_debug_dev: false

# model
model_name: ${model.name}

finetuning_mode: standard # only for logging: 'standard', 'lora', 'l_lora'
lora_config: null
linearized_lora: false

# data
datamodule: ${dataset.datamodule}
dataset_name: ${dataset.name}
num_workers: 12
batch_size: ??? # model.batch_size / num_gpus

#* EDITING MODELS WITH TASK ARITHMETIC. Appendix B
# Namely, we fine-tune for 2000 iterations with a batch size of 128,
# learning rate 1e-5 and a cosine annealing learning rate schedule with
# 200 warm-up steps and the AdamW optimizer [58; 75], with weight decay 0.1.
learning_rate: 0.00001
warmup_steps: 400
max_steps: 4000
weight_decay: 0.1

#* FABRIC ARGUMENTS
# https://lightning.ai/docs/fabric/stable/api/fabric_args.html
fabric:
  accelerator: gpu
  # strategy: deepspeed
  devices: 1
