defaults:
  - model: ViT-B-16
  - dataset: Cars
  - _self_

seed: 42
fast_debug_dev: false

# model
model_name: ${model.name}

finetuning_mode: standard # linear or standard, ["linear", "standard", "lora_linear", "lora_standard"]
lora_config:
  _target_: peft.LoraConfig
  target_modules:
    - v_porj
    - q_proj
  inference_mode: false
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1

# data
datamodule: ${dataset.datamodule}
dataset_name: ${dataset.name}
num_workers: 12
batch_size: ??? # model.batch_size / num_gpus

#* EDITING MODELS WITH TASK ARITHMETIC. Appendix B
# Namely, we fine-tune for 2000 iterations with a batch size of 128, 
# learning rate 1e-5 and a cosine annealing learning rate schedule with
# 200 warm-up steps and the AdamW optimizer [58; 75], with weight decay 0.1.
learning_rate: 0.00001
warmup_steps: 200
max_steps: 2000
weight_decay: 0.1
num_grad_accumulation: ${model.num_grad_accumulation}

#* FABRIC ARGUMENTS
# https://lightning.ai/docs/fabric/stable/api/fabric_args.html
fabric:
  accelerator: gpu
  # strategy: deepspeed
  devices: 1
