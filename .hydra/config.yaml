model:
  model:
    _target_: transformers.AutoModelForSeq2SeqLM.from_pretrained
    pretrained_model_name_or_path: ${..model_name_or_path}
  tokenizer:
    _target_: transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: ${..model_name_or_path}
  model_name_or_path: google/flan-t5-base
  name: flan-t5-base
  tokenizer_kwargs:
    padding: max_length
    truncation: true
    return_tensors: pt
  linearize: false
peft:
  peft_config: null
dataset:
  name: glue-rte
  datasets:
    _target_: datasets.load_dataset
    path: glue
    name: rte
  preprocessor:
    _target_: peta.tasks.RTE_Preprocessor
  map_kwargs:
    remove_columns:
    - sentence1
    - sentence2
    - label
    - idx
    batched: true
    num_proc: 1
    desc: Running tokenizer on dataset
optim:
  optimizer:
    _target_: torch.optim.Adam
    lr: 1.0e-05
    weight_decay: 0
seed: 42
batch_size: 16
num_workers: 8
trainer:
  accelerator: gpu
  devices:
  - 0
  - 1
  - 2
  - 3
  max_epochs: null
  max_steps: 2000
  accumulate_grad_batches: 1
  profiler: simple
  enable_checkpointing: false
  fast_dev_run: false
