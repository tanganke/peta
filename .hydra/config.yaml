model:
  model:
    _target_: transformers.AutoModelForSeq2SeqLM.from_pretrained
    pretrained_model_name_or_path: ${..model_name_or_path}
  tokenizer:
    _target_: transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: ${..model_name_or_path}
  model_name_or_path: google/flan-t5-base
  tokenizer_kwargs:
    padding: max_length
    truncation: true
    return_tensors: pt
peft:
  peft_config:
    _target_: peft.LoraConfig
    target_modules:
    - q
    - v
    inference_mode: false
    r: 16
    lora_alpha: 32
    lora_dropout: 0.1
  seed: 42
dataset:
  datasets:
    _target_: datasets.load_dataset
    path: glue
    name: cola
  preprocessor:
    _target_: peta.tasks.CoLA_Preprocessor
  map_kwargs:
    remove_columns:
    - sentence
    - label
    - idx
    batched: true
    num_proc: 1
    desc: Running tokenizer on dataset
optim:
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
seed: 42
batch_size: 32
num_workers: 8
trainer:
  accelerator: gpu
  devices: 4
  max_epochs: 1
  accumulate_grad_batches: 1
  profiler: simple
  fast_dev_run: false
