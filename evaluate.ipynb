{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hydra\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "import peft\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, default_data_collator\n",
    "\n",
    "import peta\n",
    "from peta.models.LinearizedModel import LinearizedModelWraper\n",
    "from peta.utils import TimeIt, TitledLog\n",
    "from peta.utils.logging.rich import pprint_yaml, setup_colorlogging\n",
    "from peta.utils.ml.devices import num_devices\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# disable tokenizers parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from finetune_lm import load_model_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_model(\n",
    "    model: str,\n",
    "    dataset: str,\n",
    "    finetune_mode: str,\n",
    "    version: int,\n",
    "):\n",
    "    log_dir: Path = (\n",
    "        Path(\"logs\") / model / dataset / finetune_mode / f\"version_{version}\"\n",
    "    )\n",
    "    config_path = log_dir / \"config.yaml\"\n",
    "    cfg: DictConfig = OmegaConf.load(config_path)\n",
    "\n",
    "    # load model from config\n",
    "    with TitledLog(\"load pretrained model and tokenizer\", log_fn=log.info):\n",
    "        _return = load_model_from_config(cfg)\n",
    "        tokenizer: AutoTokenizer = _return[\"tokenizer\"]\n",
    "        model: AutoModelForSeq2SeqLM | peft.PeftModel = _return[\"model\"]\n",
    "\n",
    "        # load checkpoint\n",
    "        checkpoint_dir = log_dir / \"checkpoints\"\n",
    "        checkpoints = os.listdir(checkpoint_dir)\n",
    "        # get checkpoint files with `step=2000.pth` in its name\n",
    "        checkpoints = list(filter(lambda x: \"step=2000.pth\" in x, checkpoints))\n",
    "        assert (\n",
    "            len(checkpoints) == 1\n",
    "        ), f\"multiple checkpoints found, found checkpoints: {checkpoints}\"\n",
    "        checkpoint = checkpoints[0]\n",
    "        log.info(f\"load checkpoint from {checkpoint}\")\n",
    "\n",
    "        # load trainable parameters\n",
    "        state_dict = torch.load(checkpoint_dir / checkpoint, map_location=\"cpu\")\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        model.eval()\n",
    "\n",
    "    return {\n",
    "        \"config\": cfg,\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:40:08] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> =======load pretrained model and <span style=\"color: #808000; text-decoration-color: #808000\">tokenizer</span>========                              <a href=\"file:///tmp/ipykernel_42254/501775352.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">501775352.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_42254/501775352.py#14\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:40:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m =======load pretrained model and \u001b[33mtokenizer\u001b[0m========                              \u001b]8;id=791281;file:///tmp/ipykernel_42254/501775352.py\u001b\\\u001b[2m501775352.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=272154;file:///tmp/ipykernel_42254/501775352.py#14\u001b\\\u001b[2m14\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:40:12] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> no peft config found, use full finetuning.                                   <a href=\"file:///data/users/tanganke/projects/task_arithmetic/peta/finetune_lm.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">finetune_lm.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///data/users/tanganke/projects/task_arithmetic/peta/finetune_lm.py#128\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">128</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:40:12]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m no peft config found, use full finetuning.                                   \u001b]8;id=46763;file:///data/users/tanganke/projects/task_arithmetic/peta/finetune_lm.py\u001b\\\u001b[2mfinetune_lm.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=258350;file:///data/users/tanganke/projects/task_arithmetic/peta/finetune_lm.py#128\u001b\\\u001b[2m128\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> load checkpoint from <span style=\"color: #808000; text-decoration-color: #808000\">epoch</span>=<span style=\"color: #800080; text-decoration-color: #800080\">3_step</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2000.</span>pth                                      <a href=\"file:///tmp/ipykernel_42254/501775352.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">501775352.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_42254/501775352.py#28\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m load checkpoint from \u001b[33mepoch\u001b[0m=\u001b[35m3_step\u001b[0m=\u001b[1;36m2000.\u001b[0mpth                                      \u001b]8;id=763378;file:///tmp/ipykernel_42254/501775352.py\u001b\\\u001b[2m501775352.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=441232;file:///tmp/ipykernel_42254/501775352.py#28\u001b\\\u001b[2m28\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> ==================================================                              <a href=\"file:///tmp/ipykernel_42254/501775352.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">501775352.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_42254/501775352.py#14\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ==================================================                              \u001b]8;id=809651;file:///tmp/ipykernel_42254/501775352.py\u001b\\\u001b[2m501775352.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=891435;file:///tmp/ipykernel_42254/501775352.py#14\u001b\\\u001b[2m14\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b24685ec7e45678b35de93a83d08c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = \"flan-t5-base\"\n",
    "dataset = \"glue-cola\"\n",
    "finetune_mode = \"standard\"\n",
    "\n",
    "model = load_finetuned_model(model, dataset, finetune_mode, 0)\n",
    "cfg, model, tokenizer = model[\"config\"], model[\"model\"], model[\"tokenizer\"]\n",
    "\n",
    "datasets = instantiate(cfg.dataset.datasets)\n",
    "\n",
    "\n",
    "if \"validation\" in datasets:\n",
    "    val_dataset = datasets[\"validation\"]\n",
    "elif \"validataion_matched\" in datasets:\n",
    "    # mnli\n",
    "    val_dataset = datasets[\"validataion_matched\"]\n",
    "else:\n",
    "    raise KeyError(datasets.keys())\n",
    "\n",
    "# convert the task to text-to-text format\n",
    "if hasattr(cfg.dataset, \"preprocessor\"):\n",
    "    preprocessor = instantiate(\n",
    "        cfg.dataset.preprocessor,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenizer_kwargs=cfg.model.tokenizer_kwargs\n",
    "        if hasattr(cfg.model, \"tokenizer_kwargs\")\n",
    "        else None,\n",
    "    )\n",
    "    val_dataset = val_dataset.map(\n",
    "        preprocessor,\n",
    "        **cfg.dataset.map_kwargs if hasattr(cfg.dataset, \"map_kwargs\") else {},\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    collate_fn=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m val_loader:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(batch)\n\u001b[1;32m      3\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(val_loader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
